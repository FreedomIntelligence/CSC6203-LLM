# CSC6203: Selected Topics in CS III (Large Language Models)

## Course Outline
- Introduction to Large Language Models (LLMs) （from a user's point of view）
- Language models and beyond（Word2vec, ELMO, BERT, GPT, GPT-3, Instruct-GPT, Code-davici, ChatGPT，GPT-4 etc.）
- Architecture engineering and scaling law: Transformer and beyond (https://github.com/karpathy/nanoGPT, position embedding, emergent abilities, long context Transformer)
- Training LLMs from scratch (Pre-training, SFT and learning LLMs with human feedback; parameter-efficient training, LORA, Parallelism, see [LLMZoo](https://github.com/FreedomIntelligence/LLMZoo) and [LLMFactory](https://github.com/FreedomIntelligence/LLMFactory))
- Efficiency in LLM. (https://github.com/karpathy/llama2.c  sparse language model, quantilization, pruning, knowledge distillation, modular architectures, etc.)
- Prompt engineering  ( in-context learning, COT and advanced prompts)
- Knowledge and Reasoning (memorization, coding)
- Multimodal LMs
- LLMs in vertical domains ([HuatuoGPT](https://github.com/FreedomIntelligence/HuatuoGPT) and [Medical-NLP](https://github.com/FreedomIntelligence/Medical_NLP), ChatLaw).
- Tools  and Large language models (embodied AI with robots, web search, plugins and beyond)
- Privacy, bias, fairness,  Toxicity and  Holistic Evalution ( Calibration and evaluation)
- Alignment and Limitations (know how to reject, rubustness, Hallucination, irrelevant context, low-resource language)
- Guest lecture (topic undecided, e.g., localized ChatGPT, Neuroscience-Inspired Artificial Intelligence)
- Guest lecture (topic undecided, e.g. mathmatical problems in LLMs, games using LLMs)
- In-class presentation (extended class)

## Teaching Assistant

- Xidong Wang (Leading TA)
- Fei Yu
- Zhengyang Tang
- Junying Chen
- Juhao Liang
- Zhuoheng Ma

##  Course Project

All projects will be archived  and open-sourced. Please specify reasons if you decide not to do so.

### Default Projects

### Customized Projects

### Sponsored Projects
[Call for Sponsorship]()

## Awards
- Best Research Award
- Best Presentation Award
- Best Poster Award
- Promising Research Award
- TA favorites

> Anyone who gets one of the above awards would likely get expected recomendations in any occasion. 

## Problems unsolved



## Bonus

- Anyone who releases a public LLM-related paper with permission from his/her supervisors could get 5 bonus marks (total marks should not exceed 100).



## References
[COS 597G (Fall 2022): Understanding Large Language Models by Danqi](https://www.cs.princeton.edu/courses/archive/fall22/cos597G/)
[CS25: Transformers United V2](https://web.stanford.edu/class/cs25/)
[CS324 - Large Language Models](https://stanford-cs324.github.io/winter2022/calendar/)




